{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d215851a-c9e8-4a1d-8d6e-ff3184838ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "\n",
    "from transformers import AdamW\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fbe509-fc7d-438b-b3a2-8dc2f7123699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "label2id = pickle.load(open('temp_results/label2id_dict.pkl','rb'))\n",
    "id2label = pickle.load(open('temp_results/id2label_lst.pkl','rb'))\n",
    "\n",
    "train_data = pd.read_csv('data/Patent14K/train.csv')\n",
    "test_data = pd.read_csv('data/Patent14K/test.csv')\n",
    " \n",
    "model = AutoModelForMaskedLM.from_pretrained(\"anferico/bert-for-patents\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"anferico/bert-for-patents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c588946-ca65-4c2e-84e3-3a2c25789f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2id_lst(str):\n",
    "    id_lst = []\n",
    "    for l in str.split(','):\n",
    "        id_lst.append(label2id[l])\n",
    "    return id_lst\n",
    "\n",
    "class PatentDataset(Dataset):\n",
    "    def __init__(self,df,labeled = True):\n",
    "        self.df = df\n",
    "        self.labeled = labeled\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.iloc[idx]['text'][3:]\n",
    "        label = str2id_lst(self.df.iloc[idx]['cpc_ids'])\n",
    "        \n",
    "        if self.labeled:\n",
    "            return text,label\n",
    "        else:\n",
    "            return text,None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19338e1-e76d-48f3-a089-0ba440daca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    \n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                      truncation=True,\n",
    "                                      padding='max_length',\n",
    "                                      max_length=500,\n",
    "                                      return_tensors='pt',\n",
    "                                      return_length=True)\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    \n",
    "    batch_label = np.zeros((len(labels),len(id2label)))\n",
    "    for i,_label in enumerate(labels):\n",
    "        batch_label[i,_label]=1\n",
    "    \n",
    "    batch_label = torch.tensor(batch_label,dtype=torch.float32)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, batch_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf86d0d-b738-4e82-aa00-9d0dc16447c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PatentDataset(train_data)\n",
    "test_dataset = PatentDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                             batch_size = 1,\n",
    "                             collate_fn = collate_fn)\n",
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                            batch_size = 1,\n",
    "                            collate_fn = collate_fn)\n",
    "\n",
    "train_bert_cls_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5859c-818b-435a-bb14-a8c135c72aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14164/2059636 [05:52<20:07:39, 28.23it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, token_type_ids, batch_label in tqdm(train_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        token_type_ids = token_type_ids.cuda()\n",
    "        batch_label = batch_label.cuda()\n",
    "        prediction = model(input_ids = input_ids,\n",
    "                           attention_mask = attention_mask,\n",
    "                           token_type_ids = token_type_ids,\n",
    "                           output_hidden_states=True).hidden_states[-1]\n",
    "        \n",
    "        train_bert_cls_outputs.append(prediction.squeeze(dim = 0).cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33972cd-a63b-4823-ae75-15163d663e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_bert_cls_outputs,open('outputs/bert_embedding/train_bert_cls_outputs.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d4aa3-8995-424f-b6e6-d30cc1b255b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bert_cls_outputs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, token_type_ids, batch_label in tqdm(test_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        token_type_ids = token_type_ids.cuda()\n",
    "        batch_label = batch_label.cuda()\n",
    "        prediction = model(input_ids = input_ids,\n",
    "                           attention_mask = attention_mask,\n",
    "                           token_type_ids = token_type_ids,\n",
    "                           output_hidden_states=True).hidden_states[-1]\n",
    "        \n",
    "        test_bert_cls_outputs.append(prediction.squeeze(dim = 0).cpu().detach().numpy())\n",
    "\n",
    "pickle.dump(test_bert_cls_outputs,open('outputs/bert_embedding/test_bert_cls_outputs.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba5eb1-1229-4abf-aaf5-f4db65a7fcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
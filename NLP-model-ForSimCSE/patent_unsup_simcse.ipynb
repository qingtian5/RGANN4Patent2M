{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b47b66-57af-4c51-a6cc-c3cb1699c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 219/219 [03:25<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AdamW\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda:0'\n",
    "gpus = [0,1,2,3]\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "label2id = pickle.load(open('../temp_results/mini_label2id_dict.pkl','rb'))\n",
    "id2label = pickle.load(open('../temp_results/mini_id2label_lst.pkl','rb'))\n",
    "\n",
    "train_data = pd.read_csv('../data/mini_train_data.csv')\n",
    "test_data = pd.read_csv('../data/mini_test_data.csv')\n",
    "\n",
    "\n",
    "from transformers import AutoModelForMaskedLM,AutoTokenizer,BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_path = \"anferico/bert-for-patents\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "Config = BertConfig.from_pretrained(model_path)\n",
    "Config.attention_probs_dropout_prob = 0.1\n",
    "Config.hidden_dropout_prob = 0.1\n",
    "output_way = 'pooler'\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,model_path,output_way):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_path,config=Config)\n",
    "        self.output_way = output_way\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        x1 = self.bert(input_ids = input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,output_hidden_states=True)\n",
    "        if self.output_way == 'cls':\n",
    "            output = x1.hidden_states[-1][:,0]\n",
    "        elif self.output_way == 'pooler':\n",
    "            output = x1.hidden_states[-1].mean(dim=1)\n",
    "        return output\n",
    "    \n",
    "model = NeuralNetwork(model_path,output_way)\n",
    "model = nn.DataParallel(model.to(device), device_ids=gpus, output_device=gpus[0])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "def compute_loss(y_pred,lamda=0.05):\n",
    "    idxs = torch.arange(0,y_pred.shape[0],device='cuda:0')\n",
    "    y_true = idxs + 1 - idxs % 2 * 2\n",
    "    similarities = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=2)\n",
    "    #torch自带的快速计算相似度矩阵的方法\n",
    "    similarities = similarities-torch.eye(y_pred.shape[0],device='cuda:0') * 1e12\n",
    "    #屏蔽对角矩阵即自身相等的loss\n",
    "    similarities = similarities / lamda\n",
    "    #论文中除以 temperature 超参 0.05\n",
    "    loss = F.cross_entropy(similarities,y_true)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def str2id_lst(str_label):\n",
    "    id_lst = []\n",
    "    for l in str_label.split(','):\n",
    "        id_lst.append(label2id[l])\n",
    "    return id_lst\n",
    "\n",
    "class PatentDataset(Dataset):\n",
    "    def __init__(self,df,labeled = True):\n",
    "        self.df = df\n",
    "        self.labeled = labeled\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.iloc[idx]['text'][3:]\n",
    "        label = str2id_lst(self.df.iloc[idx]['cpc_ids'])\n",
    "        \n",
    "        if self.labeled:\n",
    "            return text,label\n",
    "        else:\n",
    "            return text,None\n",
    "        \n",
    "def collate_fn(data):\n",
    "    sents = []\n",
    "    for i in data:\n",
    "        sents.append(i[0])\n",
    "        sents.append(i[0])\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        labels.append(i[1])\n",
    "        labels.append(i[1])\n",
    "    \n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                       truncation=True,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=128,\n",
    "                                       return_tensors='pt',\n",
    "                                       return_length=True)\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    \n",
    "    batch_label = np.zeros((len(labels),len(id2label)))\n",
    "    for i,_label in enumerate(labels):\n",
    "        batch_label[i,_label]=1\n",
    "    \n",
    "    batch_label = torch.tensor(batch_label,dtype=torch.float32)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, batch_label\n",
    "\n",
    "train_dataset = PatentDataset(train_data)\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = 64,\n",
    "                              collate_fn = collate_fn,\n",
    "                              shuffle = True)\n",
    "\n",
    "epochs = 1\n",
    "save_path = './patent_bert_simcse/simcsepatent_bs64.pth'\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for iter,(input_ids, attention_mask, token_type_ids, batch_label) in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        pred = model(input_ids,attention_mask,token_type_ids)\n",
    "        loss = compute_loss(pred)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(model.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc77f94-865f-4880-8e2c-4e772646524e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1e5ae3-ef56-4fc9-b8ac-3dc732798b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AdamW\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2b69c8-a177-4ae6-8476-898a57052b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = pickle.load(open('temp_results/mini_label2id_dict.pkl','rb'))\n",
    "id2label = pickle.load(open('temp_results/mini_id2label_lst.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee0f61b-0d73-4980-9dc1-02cafb9afe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/mini_train_data.csv')\n",
    "test_data = pd.read_csv('data/mini_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd782f6-d787-4088-bd8c-a7f0dc445124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"anferico/bert-for-patents\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"anferico/bert-for-patents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc731314-943f-4ee3-889c-1bd37ceea58d",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34139872-f8d7-4ccf-816b-2656e7bc838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2id_lst(str_label):\n",
    "    id_lst = []\n",
    "    for l in str_label.split(','):\n",
    "        id_lst.append(label2id[l])\n",
    "    return id_lst\n",
    "\n",
    "class PatentDataset(Dataset):\n",
    "    def __init__(self,df,labeled = True):\n",
    "        self.df = df\n",
    "        self.labeled = labeled\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.iloc[idx]['text'][3:]\n",
    "        label = str2id_lst(self.df.iloc[idx]['cpc_ids'])\n",
    "        \n",
    "        if self.labeled:\n",
    "            return text,label\n",
    "        else:\n",
    "            return text,None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd54a648-3731-4a71-94df-e405b05d1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PatentDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5037c0-9b01-4c1d-b26b-e4a3078f60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    \n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                      truncation=True,\n",
    "                                      padding='max_length',\n",
    "                                      max_length=500,\n",
    "                                      return_tensors='pt',\n",
    "                                      return_length=True)\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    \n",
    "    batch_label = np.zeros((len(labels),len(id2label)))\n",
    "    for i,_label in enumerate(labels):\n",
    "        batch_label[i,_label]=1\n",
    "    \n",
    "    batch_label = torch.tensor(batch_label,dtype=torch.float32)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, batch_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebbf43df-16cc-46a3-b9da-fbf871e9afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                            batch_size = 4,\n",
    "                            collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506f49c-923f-4b19-ab57-98cc5d67491c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96bfda09-17fa-4194-90d1-73f332a5645f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatentClsModel(nn.Module):\n",
    "    def __init__(self,bert_model,backbone_fixed = True):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.BatchNorm1d(1024),\n",
    "                               nn.Dropout(0.5),\n",
    "                               nn.Linear(1024,768),\n",
    "                               nn.ReLU(),\n",
    "                               nn.BatchNorm1d(768),\n",
    "                               nn.Dropout(0.5),\n",
    "                               nn.Linear(768,len(id2label)))\n",
    "        \n",
    "        self.bert_model = bert_model\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.backbone_fixed = backbone_fixed\n",
    "        \n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        if self.backbone_fixed:\n",
    "            with torch.no_grad():\n",
    "                x = self.bert_model(input_ids = input_ids,\n",
    "                                    attention_mask = attention_mask,\n",
    "                                    token_type_ids = token_type_ids,\n",
    "                                    output_hidden_states=True).hidden_states[-1]\n",
    "        else:\n",
    "            x = self.bert_model(input_ids = input_ids,\n",
    "                                attention_mask = attention_mask,\n",
    "                                token_type_ids = token_type_ids,\n",
    "                                output_hidden_states=True).hidden_states[-1]\n",
    "            \n",
    "        x = self.fc(x[:,0])\n",
    "        x = self.sig(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd4121-e2e7-4cc7-b606-b62c68b892ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0e619ce-202e-49b0-aa1f-5033c8407dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)\n",
    "total_epochs = 30\n",
    "test_predict_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c631b42-e179-4506-aaf3-55362e0159d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for train_index, valid_index in kfold.split(train_data,train_data['cpc_ids']):\n",
    "    \n",
    "    print('*'*20)\n",
    "    print(f'Fold{len(test_predict_lst)+1}')\n",
    "    print('*'*20)\n",
    "    train_dataset = PatentDataset(train_data.iloc[train_index])\n",
    "    val_dataset = PatentDataset(train_data.iloc[valid_index])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                 collate_fn = collate_fn,\n",
    "                                 batch_size = 4,\n",
    "                                 shuffle = True,\n",
    "                                 drop_last = True)\n",
    "    val_dataloader = DataLoader(val_dataset,\n",
    "                               collate_fn = collate_fn,\n",
    "                               batch_size = 4,\n",
    "                               shuffle = True,\n",
    "                               drop_last = True)\n",
    "\n",
    "    patentModel = PatentClsModel(model,backbone_fixed = True).cuda()\n",
    "    loss_func = nn.BCELoss()\n",
    "    optimizer = AdamW(patentModel.parameters(), lr=5e-4)\n",
    "    # reg_lambda = 0.035\n",
    "\n",
    "    print('Dataloader Success---------------------')\n",
    "\n",
    "    best_val_loss = 100\n",
    "    for epoch in range(total_epochs):\n",
    "        if epoch%5==0:\n",
    "            print('|',\">\" * epoch,\" \"*(total_epochs-epoch),'|')\n",
    "\n",
    "        patentModel.train()\n",
    "        for iter,(input_ids, attention_mask, token_type_ids, batch_label) in enumerate(tqdm(train_dataloader)):\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            token_type_ids = token_type_ids.cuda()\n",
    "            batch_label = batch_label.cuda()\n",
    "\n",
    "            prediction = patentModel(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            # l2_reg = None\n",
    "            # for w in patentModel.fc.parameters():\n",
    "            #     if not l2_reg:\n",
    "            #         l2_reg = w.norm(2)\n",
    "            #     else:\n",
    "            #         l2_reg = l2_reg + w.norm(2)\n",
    "\n",
    "            loss = loss_func(prediction,batch_label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = 0\n",
    "        patentModel.eval()\n",
    "        with torch.no_grad():\n",
    "            for iter,(input_ids, attention_mask, token_type_ids, batch_label) in enumerate(tqdm(val_dataloader)):\n",
    "                input_ids = input_ids.cuda()\n",
    "                attention_mask = attention_mask.cuda()\n",
    "                token_type_ids = token_type_ids.cuda()\n",
    "                batch_label = batch_label.cuda()\n",
    "                prediction = patentModel(input_ids, attention_mask, token_type_ids)\n",
    "                loss = loss_func(prediction,batch_label)\n",
    "                val_loss += loss.detach().item()\n",
    "            val_loss = val_loss/(iter+1)\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch {}, val_loss {:.4f}'.format(epoch, val_loss))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(patentModel.state_dict(), 'ckpt/004/best_model_mini_{}.pth'.format(len(test_predict_lst)+1))\n",
    "            print('Best val loss found: ', best_val_loss)\n",
    "\n",
    "    print('This fold, the best val loss is: ', best_val_loss)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_predict = None\n",
    "    patentModel = PatentClsModel(model,backbone_fixed = True).cuda()\n",
    "    patentModel.load_state_dict(torch.load('ckpt/004/best_model_mini_{}.pth'.format(len(test_predict_lst)+1)))\n",
    "\n",
    "    patentModel.eval()\n",
    "    with torch.no_grad():\n",
    "        for iter,(input_ids, attention_mask, token_type_ids, batch_label) in enumerate(tqdm(test_dataloader)):\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            token_type_ids = token_type_ids.cuda()\n",
    "            batch_label = batch_label.cuda()\n",
    "            prediction = patentModel(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if test_predict is None:\n",
    "                test_predict = prediction\n",
    "            else:\n",
    "                test_predict = torch.cat((test_predict,prediction),axis = 0)\n",
    "\n",
    "            loss = loss_func(prediction,batch_label)\n",
    "            test_loss += loss.detach().item()\n",
    "\n",
    "    test_loss /= (iter+1)\n",
    "    print('This fold, the test loss is: ', test_loss)\n",
    "\n",
    "    test_predict_lst.append(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa33e6-a7b8-49fd-ad32-1302e442a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_predict_lst,'outputs/mini_test_pred_lst/mini_50e_004.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8e4fd-0d6d-403c-aebc-6ef551b61ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be91e949-84bc-456f-95c5-30d964f6b919",
   "metadata": {},
   "source": [
    "# Last All Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18244b80-d3cb-4dc2-8385-cd8ea83b12ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a93185-099c-4a55-b05a-67747d9e4d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97278a8e-fec0-4f28-9627-bde4f1ad4773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307bdde-aad0-48c2-a0ba-8cce6b3d2438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378dfde-3d67-4c93-945f-9de363e5bb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e30dc-ae38-4428-bac1-3cddfc402687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0b0f62-d2b7-4309-a547-fbb347521816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patentModel.load_state_dict(torch.load('ckpt/001/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afcf1ae0-6214-487f-aed1-cfdb63b2e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(patentModel.state_dict(), 'ckpt/001/best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
